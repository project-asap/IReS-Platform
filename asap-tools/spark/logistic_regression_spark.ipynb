{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib # used to store and load models to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imr_labels(line):\n",
    "    \"\"\"\n",
    "    Parses a line of the IMR dataset and only keeps the labels\n",
    "    :param line:\n",
    "    :return: a tuple of the 2 labels\n",
    "    \"\"\"\n",
    "    sl = line.split(\";\")\n",
    "    if not sl[1].isdigit():\n",
    "        return\n",
    "    label1 = int(sl[1])\n",
    "    label2 = int(sl[2])\n",
    "    return (label1, label2)\n",
    "\n",
    "\n",
    "def parse_imr_line(line):\n",
    "    \"\"\"\n",
    "    Parses a line of the IMR csv dataset to tupples\n",
    "    :param line:\n",
    "    :return: ( (label1 (int), label2 (int)), features(list of float) )\n",
    "    \"\"\"\n",
    "    sl = line.split(\";\")\n",
    "    if not sl[1].isdigit():\n",
    "        return\n",
    "\n",
    "    label1 = int(sl[1])\n",
    "    label2 = int(sl[2])\n",
    "    features = map(float, sl[3:])\n",
    "    return ((label1, label2), features)\n",
    "\n",
    "def parse_imr_line_encoding_labels(L1_encoder, L2_encoder, line):\n",
    "    \"\"\"\n",
    "    Parses a line of the IMR csv dataset to to encoded tupples (with consecutive class IDs)\n",
    "    :param L1_encoder: the LabelEncoder for the first label\n",
    "    :param L2_encoder:  the LabelEncoder for the second label\n",
    "    :param line:\n",
    "    :return: ( (label1 (int), label2 (int)), features(list of float) )\n",
    "    \"\"\"\n",
    "    rv = parse_imr_line(line)\n",
    "    if rv is None : return\n",
    "\n",
    "    (label1, label2), features = rv[0], rv[1]\n",
    "    l1, l2 = L1_encoder.transform(label1), L2_encoder.transform(label2)\n",
    "    return ( (l1, l2) , features)\n",
    "\n",
    "def create_labeled_point( labels_and_features, wanted_category):\n",
    "    \"\"\"\n",
    "    Parses the line using the parser function lambda, and creates a LabeledPoing with\n",
    "    the 'wanted' category as label\n",
    "    :param line: the line to parse\n",
    "    :param parser_function: the lambda function that creates the tuples\n",
    "    :param line: the string line to parse    \n",
    "    \"\"\"\n",
    "    labels = labels_and_features[0]\n",
    "    features = labels_and_features[1]\n",
    "    \n",
    "    return LabeledPoint(labels[wanted_category], features)      \n",
    "\n",
    "def get_labels_from_csv(raw_data_rrd):\n",
    "    \"\"\"\n",
    "    Given an imr csv file, returns the set of unique cat1 and cat2 labels in that file\n",
    "    :param fname: The path to the csv file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label_tuples_rrd = raw_data_rrd.map(get_imr_labels).filter(lambda line: line is not None)\n",
    "    l1_rdd = label_tuples_rrd.map(lambda (l1,l2): l1)\n",
    "    l2_rdd = label_tuples_rrd.map(lambda (l1,l2): l2)\n",
    "    labels_1 = l1_rdd.distinct().collect()\n",
    "    labels_2 = l2_rdd.distinct().collect()\n",
    "    return labels_1, labels_2\n",
    "\n",
    "def create_label_encoders(input_csv_file):\n",
    "    labels_1, labels_2 =  get_labels_from_csv(input_csv_file)\n",
    "    L1_encoder = LabelEncoder();L1_encoder.fit(labels_1)\n",
    "    L2_encoder = LabelEncoder();L2_encoder.fit(labels_2)\n",
    "    return L1_encoder, L2_encoder\n",
    "\n",
    "def store_label_encoders(enc1, enc2, le_path):\n",
    "    joblib.dump( (enc1, enc2), le_path)\n",
    "    \n",
    "def load_label_encoders(le_path):\n",
    "    (l1e, l2e) = joblib.load(le_path)\n",
    "    return l1e, l2e\n",
    "\n",
    "def calculate_error(data_rrd, model):\n",
    "    # Evaluating the model on train data\n",
    "    labelsAndPreds = data_rrd.map(lambda p: (p.label, model.predict(p.features) ))\n",
    "    err = labelsAndPreds.filter(lambda (l, p): l != p).count() / float(data_rrd.count())\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_counts:  5789 52\n"
     ]
    }
   ],
   "source": [
    "in_file = \"/home/cmantas/Data/Result_W2V_IMR_New.csv\"\n",
    "encoders_path= \"/home/cmantas/Data/spark_lr_data/preprocess/label_encoders\"\n",
    "\n",
    "raw_data = sc.textFile(in_file)\n",
    "\n",
    "l1e, l2e =  create_label_encoders(raw_data)\n",
    "\n",
    "\n",
    "store_label_encoders(l1e, l2e, encoders_path)\n",
    "l1e, l2e = load_label_encoders(encoders_path)\n",
    "\n",
    "classes_1_count = len(l1e.classes_); classes_2_count = len(l2e.classes_)\n",
    "\n",
    "print \"Class_counts: \", classes_1_count, classes_2_count\n",
    "\n",
    "# lambda closure including the encoders\n",
    "encoding_mapper = lambda line: parse_imr_line_encoding_labels(l1e, l2e, line)\n",
    "\n",
    "encoded_tupple_data = raw_data.map(encoding_mapper).filter(lambda l: l is not None)\n",
    "\n",
    "#lambda closure for creating a Labeled Points for the first category\n",
    "labeled_point_1_mapper = lambda tupples: create_labeled_point(tupples, 0)\n",
    "\n",
    "# the final RRD of Labaled points\n",
    "labeled_data_1 = encoded_tupple_data.map(labeled_point_1_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegressionWithLBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(*args, **kwargs):\n",
    "    return LogisticRegressionWithLBFGS.train(*args, regType=None, intercept=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into 2 training datasets and 1 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training1Data, training2Data, testData) = labeled_data_1.randomSplit([0.4, 0.4, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with 1/2 of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training(1) Error:  0.0\n",
      "Training(2) Error:  0.527777777778\n",
      "Test Error:  0.461538461538\n"
     ]
    }
   ],
   "source": [
    "# Build the model with first half of training\n",
    "model1 = train(training1Data, numClasses=classes_1_count)\n",
    "print \"Training(1) Error: \", calculate_error(training1Data, model1)\n",
    "print \"Training(2) Error: \", calculate_error(training2Data, model1)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "print \"Test Error: \", calculate_error(testData, model1)\n",
    "\n",
    "# # Save and load model\n",
    "# model.save(sc, \"myModelPath\")\n",
    "# sameModel = LogisticRegressionModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with 2nd half of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training(1) Error:  0.135135135135\n",
      "Training(2) Error:  0.0\n",
      "Test Error:  0.461538461538\n"
     ]
    }
   ],
   "source": [
    "# Build a new model based on the 1st one and training with more data\n",
    "model2 = train(training2Data, initialWeights=model1.weights, numClasses=classes_1_count)\n",
    "print \"Training(1) Error: \", calculate_error(training1Data, model2)\n",
    "print \"Training(2) Error: \", calculate_error(training2Data, model2)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "print \"Test Error: \", calculate_error(testData, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with both halves of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training(1) Error:  0.0\n",
      "Training(2) Error:  0.0\n",
      "Test Error:  0.423076923077\n"
     ]
    }
   ],
   "source": [
    "## buld yet another model with both training datasets\n",
    "all_training=training1Data.union(training2Data)\n",
    "\n",
    "model3 = train(all_training, numClasses=classes_1_count)\n",
    "print \"Training(1) Error: \", calculate_error(training1Data, model3)\n",
    "print \"Training(2) Error: \", calculate_error(training2Data, model3)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "print \"Test Error: \", calculate_error(testData, model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with new splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training(1) Error:  0.027027027027\n",
      "Training(2) Error:  0.0277777777778\n",
      "Test Error: (new) 0.166666666667\n"
     ]
    }
   ],
   "source": [
    "(training_new, test_new) = labeled_data_1.randomSplit([0.8, 0.2])\n",
    "model4 = train(training_new, numClasses=classes_1_count)\n",
    "print \"Training(1) Error: \", calculate_error(training1Data, model4)\n",
    "print \"Training(2) Error: \", calculate_error(training2Data, model4)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "print \"Test Error: (new)\", calculate_error(test_new, model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.4.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}